---
layout: post
title: "电商用户标签服务系统建设及演进"
description: "电商背景下用户标签服务系统的建设及演进过程详解"
category: user-profile
tags: [user-profile]
---

用户标签服务系统，为公司用户的个性化运营提供基础数据服务，唯品会的用户标签服务系统，目前已全面服务于唯品会的各种个性化运营场景，包括站外广告定向人群投放，站内个性化运营玩法，用户触达个性化沟通，提供包括用户的基础人口属性标签，设备属性标签，用户兴趣标签，用户行为标签，以及用户的会员属性标签等，为公司的千人千面策略提供最基础的服务。现在回溯这套系统的建设及演进，再结合业务的发展变化，整个演进过程可分为三个阶段，本文将逐一进行介绍。

### 用户标签服务的0-1

其实在用户标签服务这个系统诞生之前，公司存在多个提供用户标签数据的系统，有些标签是由A系统提供，有些由B系统提供，同时根据数据最终提供的形式的不同，又分别存在以人群包为维度提供离线批量获取用户的系统以及将人群包标签化打到用户上，提供以用户为维度的在线标签查询的系统。因此，用户标签服务系统的搭建，一方面是整合管理散乱在各处的用户标签数据，另一方面也是对用户标签业务的统一管理。

在建设用户标签系统的初期，业务需求主要集中在基于已经存在的用户标签基础上，可以通过各类规则组装出业务需要的人群，进行精准营销。比如针对华南区的用户做一次美妆活动，业务需要找出最近一个月有收藏过美妆品类商品或者最近一个月有浏览过美妆品类商品的华南区女性用户，对她们进行push营销，同时在站内针对这群用户投入相应的广告位来吸引她们。

围绕业务的需求场景，用户标签系统初期的建设主要围绕：

* 用户基础标签清洗和加工
* 业务运营标签规则计算
* 标签存储
* 标签查询

系统整体架构如下：

![Architecture Overview](https://raw.githubusercontent.com/Neway6655/neway6655.github.com/master/images/user-profile-system/Architecture_overview.png)

##### 用户基础标签清洗加工

根据不同的场景需求以及数据本身的时效性特点，目前基础标签的清洗加工分为离线和实时两套处理方式。离线处理主要通过Hive/Spark以天为单位进行数据清洗和加工；实时处理则通过Kafka接入业务实时数据源，再由Storm流式处理，实时清洗和加工。（待补充：时效性保证问题）

##### 业务运营标签规则计算

基于用户的基础标签，业务方从运营的角度可筛选各类日常运营的标签，例如最近一周有浏览Nike运动鞋但没有购买的人，给这群人打上标签后，对这群人便能进行精准推送Push和投放站内外的广告等。

标签规则计算模块通过一套标签规则解析引擎，将标签规则转化为底层的执行SQL提交到Hive，完成标签的计算任务。

（待补充：数据准确性问题，差异比较，人群多版本，出错数据回滚）

##### 标签存储及查询

通过标签规则计算的标签结果，最终以用户ID List的形式存储在Hive表里，鉴于公司用户规模比较庞大，最终一个标签的用户数量往往有百万级，多的甚至达到千万级。由于公司所有的大数据资源是共享的，数据从Hive表里读取的过程有时会受到集群资源不足或者平台不稳定等因素的影响，读取大量的数据，过程中经常会失败，因此，考虑到标签查询服务的稳定性，系统内部会先将标签结果拉取到Redis，最终查询接口通过Redis里的标签数据提供快速稳定的查询服务。

标签查询服务从业务使用的方式上可划分为两类：A）以标签为维度，将符合该标签的所有用户都取出来，业务使用方式以批量发送Push/短信为主；B）以用户为维度，查询该用户身上是否符合某个标签或多个标签，业务使用方式包括广告投放，站内个性化运营等。A是离线批量查询，B是在线实时查询，在标签存储上对这两类以不同的形式分开存储。

首先，将Hive表中的标签结果用户ID List数据同步到Redis中，也是以*List*的形式存储，为了避免产生Redis的大Key，以及提高数据同步的并行度，这里的*List*是逻辑上的，实际存储时则是以一段段的小*List*拼接而成，同时，为了节省Redis的存储空间，将多个User的ID拼接作为List的一个element：

![User List Storage](https://raw.githubusercontent.com/Neway6655/neway6655.github.com/master/images/user-profile-system/user_list.png)

接着，为了提供用户维度的标签查询，需要对用户打标签，在上面的生产的Redis List数据中，对该标签的每个user进行遍历，以用户id为key，标签id为field，标签值（符合/不符合该标签）为value构造Redis的hash结构。对用户标签的查询，采用Redis针对hash结构的hmget操作即可获取某个用户的一个或多个标签的值。

对业务会经常使用的标签，数据每天都会更新，但变化的差异往往比较小，因此为了提升用户打标签的速度，在同步标签结果时，系统会生成多一份该标签的用户id bitset，通过和昨天的数据bitset进行差集比较，可快速找出每天变化的用户，进行增量打标，代替原先的全量打标过程，可大幅提高打标速度。

对于一部分标签，业务对数据的可靠性和准确性的要求非常高，使用Redis作为存储的方案存在一定的风险，因此，针对这部分标签，采用MySQL存储，Redis出现问题时，回源到MySQL进行查询。在MySQL的表结构设计上，没有采用以用户id为主键的方式，主要是考虑到数据的更新是以标签为维度，对用户进行批量更新，而一个用户也可能同时存在多个标签中，这样会导致数据的更新逻辑复杂，无法对用户批量更新，数据更新速度慢；因此，同时考虑到数据更新的问题和标签查询的性能，最终采用以标签id为主键，沿用上面bitset的思路，将该标签的用户id构成的bitset顺序等长切分为多个bitset_segment：

![User List Storage](https://raw.githubusercontent.com/Neway6655/neway6655.github.com/master/images/user-profile-system/user_tag_segments.png)



假设bitset的总长度有5亿，就代表最多可以表示5亿用户，例如上图中，符合标签A的有u2，u871，u3188等，根据每个用户的序号，可以计算出每个用户所属的bit index，在标签A的bitset中对应位置设为1，这是逻辑上的bitset，实际存储时会将bitset顺序切成多个bitset segments，假设有50个segments，那么每个segment就1千万个bit，而每个用户所属的segment index以及segment中的bit index也是可以由用户的序号计算出来的，这样对用户标签的查询操作，更新操作，都转为标签id+segment_id进行操作。



### 用户画像分析引擎--天眼

在满足用户个性化标签运营需求后，为了进一步让业务方在进行个性化标签筛选时，能更清晰直观的了解筛选出的人群构成，系统提供了一套用户画像分析引擎**天眼**，可实时地查看人群画像构成，例如：人群数量规模，性别/年龄/地域分布，品牌品类购买分布，近期访问UV趋势等。

这是2.0的系统整体架构图：

![Architecture Overview 2.0](https://raw.githubusercontent.com/Neway6655/neway6655.github.com/master/images/user-profile-system/Architecture_overview_v2.png)

为了能实时并且准确的提供用户画像分析能力，采用Elasticsearch（ES）作为分析引擎，利用其快速的查询性能和数据聚合能力。另外，由于用户的数据规模庞大，考虑到ES索引存储以及数据更新的压力，放入到ES的索引数据为抽样用户的数据，在满足数据准确性的同时，尽量降低ES的压力。

##### 索引数据结构设计

一个用户对应一个doc，用户的标签信息是doc fields，简单的标签例如性别，年龄，直接使用普通的fields即可；一些复杂的标签信息，例如用户对品牌A的最近一个月浏览次数这类，属于用户和品牌一对多的动态标签信息，需要采用nested fields，一个简单的示例结构如下：

``` json
{
  //normal fields
  "basic_info": {
    "age": 20,
    "gender": "female"
  },
  //nested fields
  "brand_info": [
    {
      "brand_id": "123456",
      "brand_last_purchase_date": "2018-01-01",
      "brand_30d_visit_cnt": 10
    },
    {
      "brand_id": "67890",
      "brand_last_purchase_date": "2018-01-05",
      "brand_30d_visit_cnt": 16
    }
  ]
}
```

##### 用户标签抽样表设计

采用id取模的方式对用户进行抽样，可确保抽样用户固定。同时，抽样用户的标签信息由多张Hive表存储，例如基础信息表，品牌行为信息表，品类行为信息表等，分别属于不同的Hive表，有的是和用户成1:1 关系的，有的是和用户成1:n关系的，例如品牌行为信息表。在处理这些表的数据时，也会做一些小优化，例如时间类型的统一采用日期形式，这样可以节省ES的存储空间。

##### 索引数据更新机制

抽样的用户标签数据在Hive表中完成更新后，需要再同步刷到ES中，数据刷新的机制经历过多次的优化迭代，最开始通过Hive表中的数据对全量抽样用户构造完整的用户json数据，再通过一个Job将ES中的数据全覆盖更新一遍，更新过程中会发现ES机器的cpu非常高，主要原因是每个用户的json数据特别大，ES在反序列化时CPU压力很大；最后优化为将全量更新调整为用户级别增量更新，并且由原来的完整json改为仅对有变化的fields按Hive表拆分各个不同fields group构造json，分批部分更新，采用小步快跑的方式完成索引数据的更新。

#####Query构造

用户的基础标签数据都已推到ES中了，那如何将各种条件组合后的业务标签逻辑，以及需要查看的指标数据，转化为ES的query语句呢？

### 智选标签

回顾系统在前面1.0和2.0两个阶段，都是围绕业务筛选人群，建立标签而建设的，而业务如何建立标签，都是为了找出符合某些运营场景的人群，实现运营目标的最大化。例如前面例子中描述的“最近一周有浏览Nike运动鞋但没有购买的人”，背后的需求其实是找出“最近对Nike运动鞋有购买倾向的人”，放入到对应的广告/Push等运营活动中，达成活动的运营目标，可能是销售额，转化率，回访率等，至于如何找出最近有购买倾向的人呢？根据经验，可以大概认为“最近一周有浏览过但又没有购买的人”是一个潜在的购买客群，但并不是最精准的人群。

若想找出精准的人群，则需要借助机器学习模型，通过大量的数据训练得到，通过条件规则建立标签，是1.0和2.0两个阶段系统建设的核心目标，而通过机器学习模型的方式建立标签--智选标签，则是系统3.0的核心目标。

3.0的系统整体架构图：

![Architecture Overview 3.0](https://raw.githubusercontent.com/Neway6655/neway6655.github.com/master/images/user-profile-system/Architecture_overview_v3.png)

* 用户特征池：为标签模型构造用户全方位的特征数据，包括用户基础信息特征，用户设备信息特征，用户访问/搜索/购买等行为特征，用户品牌/品类行为特征等。
* 标签模型：基于Spark ML的机器学习模型和Tensorflow的深度模型，从业务运营场景抽象出用户标签需求建立模型。
* 智选标签：根据运营场景需求，选择合适的模型构造用户标签。

标签建模的整体工作流如下：

![Architecture Overview 3.0](https://raw.githubusercontent.com/Neway6655/neway6655.github.com/master/images/user-profile-system/model_tagging_workflow.png)